# -*- coding: utf-8 -*-
"""Homework 5: Batch _LV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lUri9X8Y39Yh2Hy2UpwxrpHQjmPjCU0m
"""

!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet

"""# **Question 1: Install Spark and PySpark**"""

!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz
!tar xf spark-3.2.3-bin-hadoop3.2.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.3-bin-hadoop3.2"

import findspark
findspark.init()
from pyspark.sql import SparkSession

# Create a local Spark session
spark = SparkSession.builder.master("local[*]").appName("ColabSpark").getOrCreate()

print(spark.version)

"""# **Question 2: Yellow October 2024**"""

df = spark.read.parquet('yellow_tripdata_2024-10.parquet')
df = df.repartition(4)
df.write.parquet('homework/2')

"""# **Question 3: Count records**"""

from pyspark.sql.functions import col, to_date

# Filter trips that started on October 15th
df_filtered = df.filter(to_date(col("tpep_pickup_datetime")) == "2024-10-15")

# Count the number of trips
trip_count = df_filtered.count()

print(f"Number of taxi trips on October 15th: {trip_count}")

"""# **Question 4: Longest trip**"""

from pyspark.sql.functions import col, unix_timestamp, max

df_with_duration = df.withColumn(
    "trip_duration_hours",
    (unix_timestamp(col("tpep_dropoff_datetime")) - unix_timestamp(col("tpep_pickup_datetime"))) / 3600
)

longest_trip = df_with_duration.select(max("trip_duration_hours")).collect()[0][0]
print(f"Longest trip duration: {longest_trip} hours")

"""# **Question 6: Least frequent pickup location zone**"""

!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv

zone_df = spark.read.csv("taxi_zone_lookup.csv", header=True, inferSchema=True)
zone_df.createOrReplaceTempView("zones")

df.createOrReplaceTempView("yellow_taxi")

from pyspark.sql.functions import count

least_frequent_pickup = (
    df.groupBy("PULocationID")
    .agg(count("*").alias("pickup_count"))
    .join(zone_df, df.PULocationID == zone_df.LocationID, "left")
    .orderBy("pickup_count")
    .limit(1)
)

least_frequent_pickup.show()