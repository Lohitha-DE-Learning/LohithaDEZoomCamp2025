# -*- coding: utf-8 -*-
"""Copy of DEZoomcamp: dlt - Homework-LV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A10kGYq5xPjkbAj6PyudWGSwaUFi4B7F

# **Workshop "Data Ingestion with dlt": Homework - Lohitha Vanteru**

---

## **Dataset & API**

We‚Äôll use **NYC Taxi data** via the same custom API from the workshop:

üîπ **Base API URL:**  
```
https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api
```
üîπ **Data format:** Paginated JSON (1,000 records per page).  
üîπ **API Pagination:** Stop when an empty page is returned.

## **Question 1: dlt Version**

1. **Install dlt**:
"""

!pip install dlt[duckdb]

!dlt --version

"""or:"""

import dlt
print("dlt version:", dlt.__version__)

"""## **Question 2: Define & Run the Pipeline (NYC Taxi API)**

Use dlt to extract all pages of data from the API.

Steps:

1Ô∏è‚É£ Use the `@dlt.resource` decorator to define the API source.

2Ô∏è‚É£ Implement automatic pagination using dlt's built-in REST client.

3Ô∏è‚É£ Load the extracted data into DuckDB for querying.


"""

import dlt
from dlt.sources.helpers.rest_client import RESTClient
from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator

@dlt.resource(name="rides")
def taxi_data_resource():
    client = RESTClient(
        base_url="https://us-central1-dlthub-analytics.cloudfunctions.net/data_engineering_zoomcamp_api",
        paginator=PageNumberPaginator(
            base_page=1,  # API uses 1-based page numbering (page 1, 2, 3...)
            page_param="page",  # Matches the API's page parameter
            total_path=None,  # API doesn't provide total pages in response
            stop_after_empty_page=True  # Stop when empty page is received
        )
    )

    # Get paginated data (path is empty since base_url contains full endpoint)
    for page in client.paginate(""):
        yield from page  # Flatten page items

pipeline = dlt.pipeline(
    pipeline_name="ny_taxi_pipeline",
    destination="duckdb",
    dataset_name="ny_taxi_data"
)

"""Load the data into DuckDB to test:





"""

load_info = pipeline.run(taxi_data_resource())
print(load_info)

"""Start a connection to your database using native `duckdb` connection and look what tables were generated:"""

import duckdb
from google.colab import data_table
data_table.enable_dataframe_formatter()

# A database '<pipeline_name>.duckdb' was created in working directory so just connect to it

# Connect to the DuckDB database
conn = duckdb.connect(f"{pipeline.pipeline_name}.duckdb")

# Set search path to the dataset
conn.sql(f"SET search_path = '{pipeline.dataset_name}'")

# Describe the dataset
conn.sql("DESCRIBE").df()

"""## **Question 3: Explore the loaded data**

Inspect the table `ride`:
"""

df = pipeline.dataset(dataset_type="default").rides.df()
df

"""**Answer:**
* What is the total number of records extracted?
"""

print("Total records:", len(df))

"""## **Question 4: Trip Duration Analysis**

Run the SQL query below to:

* Calculate the average trip duration in minutes.
"""

with pipeline.sql_client() as client:
    res = client.execute_sql(
            """
            SELECT
            AVG(date_diff('minute', trip_pickup_date_time, trip_dropoff_date_time))
            FROM rides;
            """
        )
    # Prints column values of the first row
    print(res)